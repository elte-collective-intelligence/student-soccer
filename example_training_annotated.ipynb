{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2544bb6",
   "metadata": {},
   "source": [
    "\n",
    "#  VMAS Soccer Environment Overview\n",
    "\n",
    "In this notebook we use the **VMAS Soccer** scenario as a simple example to demonstrate how to train a multi-agent RL algorithm in a continuous-control environment.\n",
    "\n",
    "##  Basic Environment Description\n",
    "\n",
    "- We use the **Soccer** scenario from the VMAS library.\n",
    "- By default there are **3 agents on the blue team**.\n",
    "- These three blue agents cooperate to score goals against a **red team**.\n",
    "- The red team is, by default, controlled by a **built-in scripted AI opponent** (not by our learning algorithm).\n",
    "- The environment is wrapped in TorchRL and returns data as a **TensorDict**, with keys grouped by agent team.\n",
    "\n",
    "This setup is useful to:\n",
    "- Show how to plug a multi-agent algorithm into VMAS + TorchRL.\n",
    "- Let students focus first on **training only one team (blue)** against a fixed AI opponent.\n",
    "- Introduce the idea of **centralised critic / decentralised actors** in a simple, visual domain.\n",
    "\n",
    "---\n",
    "\n",
    "## `ai_red_team` Flag and `agent_red` Key\n",
    "\n",
    "The Soccer scenario exposes a configuration flag, typically passed when creating the environment:\n",
    "\n",
    "```python\n",
    "ai_red_team = True  # or False\n",
    "```\n",
    "\n",
    "### Case 1 – `ai_red_team = True` (default)\n",
    "\n",
    "- The **red team is controlled by a hand-crafted AI** inside the environment.\n",
    "- Our learning algorithm controls only the **blue agents**.\n",
    "- The TensorDict then only contains the **blue group**:\n",
    "  - `agent_blue/observation`\n",
    "  - `agent_blue/action`\n",
    "  - `agent_blue/reward`\n",
    "  - `agent_blue/terminated`, `agent_blue/truncated`, etc.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2 – `ai_red_team = False` (both teams learnable)\n",
    "\n",
    "If we set:\n",
    "\n",
    "```python\n",
    "ai_red_team = False\n",
    "```\n",
    "\n",
    "then the environment **no longer controls the red team with a built-in AI**.  \n",
    "Instead, the red team becomes a second *learnable* group of agents.\n",
    "\n",
    "In this case, the environment's TensorDict gains a **new group key**:\n",
    "\n",
    "- `agent_blue` – the original blue team (3 agents).\n",
    "- `agent_red` – the red team, now exposed like another agent group.\n",
    "\n",
    "That means we now have keys like:\n",
    "\n",
    "- `agent_red/observation`\n",
    "- `agent_red/action`\n",
    "- `agent_red/reward`\n",
    "- `agent_red/terminated`, `agent_red/truncated`, etc.\n",
    "\n",
    "This is exactly where a **second algorithm instance** can be plugged in if we want the teams to **play against each other with learned policies**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Using Two Algorithms (Blue vs Red)\n",
    "\n",
    "A typical two-sided training setup could look like this:\n",
    "\n",
    "- One algorithm instance (e.g., `MADDPG`) is configured for the **blue team**, using the group key `agent_blue`.\n",
    "- A second algorithm instance (possibly the same class, possibly different) is configured for the **red team**, using the group key `agent_red`.\n",
    "- During rollout:\n",
    "  - Both policies are called: one to compute `agent_blue/action`, one to compute `agent_red/action`.\n",
    "  - The environment uses both actions to step the dynamics.\n",
    "- During training:\n",
    "  - The replay buffer stores both groups in the same TensorDict.\n",
    "  - Each algorithm reads only the group it is responsible for:\n",
    "    - Blue algorithm reads from the `agent_blue` keys.\n",
    "    - Red algorithm reads from the `agent_red` keys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from mappo import MAPPO\n",
    "from maddpg import MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
    "n_iters = 10  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14635b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000  # Episode steps before done\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized envs. frames_per_batch should be divisible by this number\n",
    "scenario_name = \"football\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad261f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(num_envs=10,device=vmas_device,max_steps=max_steps,n_agents=4,ai_red_agents = True):\n",
    "    '''\n",
    "    ai_red_agents: If True, the red team is controlled by built-in AI agents. If False, both teams are controlled by learning agents.\n",
    "                   In the latter case, the corresponding spaces and keys in the tensordict will be under the agent_red namespace in the tensordict.\n",
    "    '''\n",
    "    return TransformedEnv(VmasEnv(\n",
    "        scenario=scenario_name,\n",
    "        num_envs=num_envs,\n",
    "        continuous_actions=True,\n",
    "        max_steps=max_steps,\n",
    "        device=device,\n",
    "        n_agents=n_agents,\n",
    "        ai_red_agents = ai_red_agents,\n",
    "    ),\n",
    "    RewardSum(in_keys=[('agent_blue', \"reward\")], out_keys=[('agent_blue', \"episode_reward\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(num_envs=num_vmas_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56180d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = MAPPO(\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    lmbda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    entropy_eps=0.001,\n",
    "    max_grad_norm=1.0,\n",
    "    total_frames=total_frames,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    minibatch_size=512,\n",
    "    num_epochs=30,\n",
    "    independent_critic=False,\n",
    "    share_parameters=False,\n",
    "    continuous_actions=False,\n",
    "    base_state_value_key=\"state_value\",\n",
    "    team_reward_aggregation_fn = \"mean\",\n",
    "    base_actor_logits_keys=\"logits\",\n",
    "    # base_actor_logits_keys=[\"loc\",\"scale\"],\n",
    "\n",
    "    do_absolute_evaluation=True,\n",
    "    evaluation_interval = 100000,\n",
    "    evaluation_episodes = 10,\n",
    "    evaluation_max_steps = 300,\n",
    "    n_agents = 3,\n",
    "    \n",
    "    \n",
    "    group = \"agent_blue\",\n",
    "    \n",
    "    root_folder = \"./experiments/mappo_student_soccer\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63706df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.setup(make_env_fn=make_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5350274",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = algorithm.train()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = MADDPG(\n",
    "    learning_rate = 3e-4,       # override if supplied,\n",
    "    gamma = 0.99,\n",
    "    max_grad_norm = 1.0,\n",
    "    polyak_tau = 0.995,\n",
    "    memory_size = 1_000_000,\n",
    "    #Exploration\n",
    "    sigma_init = 0.9,\n",
    "    sigma_end = 0.1,\n",
    "    annealing_steps_ratio = 0.5,\n",
    "    \n",
    "    #Loss\n",
    "    discrepancy_loss = \"l2\",\n",
    "    use_critic_target_network = True,\n",
    "    use_policy_target_network = False,\n",
    "    # Training loop\n",
    "    total_frames = 60_000,\n",
    "    frames_per_batch = 6_000,\n",
    "    minibatch_size = 400,\n",
    "    num_epochs = 50,\n",
    "\n",
    "    do_absolute_evaluation=True,\n",
    "    evaluation_interval = 100000,\n",
    "    evaluation_episodes = 10,\n",
    "    evaluation_max_steps = 300,\n",
    "    n_agents = 3,\n",
    "    \n",
    "    \n",
    "    group = \"agent_blue\",\n",
    "    \n",
    "    root_folder = \"./experiments/maddpg_student_soccer\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.setup(make_env_fn=make_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = algorithm.train()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-soccer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
